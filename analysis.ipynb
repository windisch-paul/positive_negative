{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trials_with_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill all empty cells in column \"Annotation_disagreement\" with 0\n",
    "df[\"Annotation_disagreement\"] = df[\"Annotation_disagreement\"].fillna(0)\n",
    "\n",
    "# Fill all empty cells in column \"Disagreement_reason\" with \"No disagreement\"\n",
    "df[\"Disagreement_reason\"] = df[\"Disagreement_reason\"].fillna(\"No disagreement\")\n",
    "\n",
    "# Rename POSITIVE to Positive and NEGATIVE to Negative\n",
    "df = df.replace({\"POSITIVE\": \"Positive\", \"NEGATIVE\": \"Negative\", \"NEGATIVE.\": \"Negative\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.to_csv(\u001b[33m\"\u001b[39m\u001b[33mtrials_with_predictions_cleaned.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"trials_with_predictions_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute annotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Annotation_disagreement\"].value_counts()\n",
    "\n",
    "print(f\"Annotators disagreed on {df['Annotation_disagreement'].value_counts()[1] / df.shape[0]:.2%} (n={df['Annotation_disagreement'].value_counts()[1]}) of trials\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number and percentages of positive and negative trials\n",
    "print(df[\"Annotation_accept\"].value_counts(normalize=False))\n",
    "print(df[\"Annotation_accept\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-3.5 Turbo predictions when reading only the conclusion and the true labels\n",
    "gpt35turbo_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt35turbo_temp100_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate true positives, false positives, false negatives, and true negatives\n",
    "gpt35turbo_conclusion_tp = gpt35turbo_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt35turbo_conclusion_fp = gpt35turbo_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt35turbo_conclusion_fn = gpt35turbo_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt35turbo_conclusion_tn = gpt35turbo_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_conclusion_accuracy = (gpt35turbo_conclusion_tp + gpt35turbo_conclusion_tn) / (gpt35turbo_conclusion_tp + gpt35turbo_conclusion_tn + gpt35turbo_conclusion_fp + gpt35turbo_conclusion_fn)\n",
    "gpt35turbo_conclusion_precision = gpt35turbo_conclusion_tp / (gpt35turbo_conclusion_tp + gpt35turbo_conclusion_fp)\n",
    "gpt35turbo_conclusion_recall = gpt35turbo_conclusion_tp / (gpt35turbo_conclusion_tp + gpt35turbo_conclusion_fn)\n",
    "gpt35turbo_conclusion_f1 = 2 * (gpt35turbo_conclusion_precision * gpt35turbo_conclusion_recall) / (gpt35turbo_conclusion_precision + gpt35turbo_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_conclusion_accuracy_ci = 1.96 * np.sqrt((gpt35turbo_conclusion_accuracy * (1 - gpt35turbo_conclusion_accuracy)) / df.shape[0])\n",
    "gpt35turbo_conclusion_precision_ci = 1.96 * np.sqrt((gpt35turbo_conclusion_precision * (1 - gpt35turbo_conclusion_precision)) / df.shape[0])\n",
    "gpt35turbo_conclusion_recall_ci = 1.96 * np.sqrt((gpt35turbo_conclusion_recall * (1 - gpt35turbo_conclusion_recall)) / df.shape[0])\n",
    "gpt35turbo_conclusion_f1_ci = 1.96 * np.sqrt((gpt35turbo_conclusion_f1 * (1 - gpt35turbo_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-3.5 Turbo Conclusion Accuracy: {gpt35turbo_conclusion_accuracy:.2f} ({gpt35turbo_conclusion_accuracy - gpt35turbo_conclusion_accuracy_ci:.2f} - {gpt35turbo_conclusion_accuracy + gpt35turbo_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Conclusion Precision: {gpt35turbo_conclusion_precision:.2f} ({gpt35turbo_conclusion_precision - gpt35turbo_conclusion_precision_ci:.2f} - {gpt35turbo_conclusion_precision + gpt35turbo_conclusion_precision_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Conclusion Recall: {gpt35turbo_conclusion_recall:.2f} ({gpt35turbo_conclusion_recall - gpt35turbo_conclusion_recall_ci:.2f} - {gpt35turbo_conclusion_recall + gpt35turbo_conclusion_recall_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Conclusion F1: {gpt35turbo_conclusion_f1:.2f} ({gpt35turbo_conclusion_f1 - gpt35turbo_conclusion_f1_ci:.2f} - {gpt35turbo_conclusion_f1 + gpt35turbo_conclusion_f1_ci:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt35turbo_conclusion = gpt35turbo_conclusion_crosstab.values.astype(str)\n",
    "labels_gpt35turbo_conclusion[0, 0] = f\"{labels_gpt35turbo_conclusion[0, 0]} ({100 * gpt35turbo_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_conclusion[0, 1] = f\"{labels_gpt35turbo_conclusion[0, 1]} ({100 * gpt35turbo_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_conclusion[1, 0] = f\"{labels_gpt35turbo_conclusion[1, 0]} ({100 * gpt35turbo_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_conclusion[1, 1] = f\"{labels_gpt35turbo_conclusion[1, 1]} ({100 * gpt35turbo_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({'font.size': 14})  # Increase font size\n",
    "sns.heatmap(gpt35turbo_conclusion_crosstab, annot=labels_gpt35turbo_conclusion, fmt=\"\", cmap=\"Blues\", annot_kws={'size': 12})\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/gpt35turbo_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-4o predictions when reading only the conclusions and the true labels\n",
    "gpt4o_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt4o_temp100_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "gpt4o_conclusion_tp = gpt4o_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt4o_conclusion_fp = gpt4o_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt4o_conclusion_fn = gpt4o_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt4o_conclusion_tn = gpt4o_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt4o_conclusion_accuracy = (gpt4o_conclusion_tp + gpt4o_conclusion_tn) / (gpt4o_conclusion_tp + gpt4o_conclusion_tn + gpt4o_conclusion_fp + gpt4o_conclusion_fn)\n",
    "gpt4o_conclusion_precision = gpt4o_conclusion_tp / (gpt4o_conclusion_tp + gpt4o_conclusion_fp)\n",
    "gpt4o_conclusion_recall = gpt4o_conclusion_tp / (gpt4o_conclusion_tp + gpt4o_conclusion_fn)\n",
    "gpt4o_conclusion_f1 = 2 * (gpt4o_conclusion_precision * gpt4o_conclusion_recall) / (gpt4o_conclusion_precision + gpt4o_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt4o_conclusion_accuracy_ci = 1.96 * np.sqrt((gpt4o_conclusion_accuracy * (1 - gpt4o_conclusion_accuracy)) / df.shape[0])\n",
    "gpt4o_conclusion_precision_ci = 1.96 * np.sqrt((gpt4o_conclusion_precision * (1 - gpt4o_conclusion_precision)) / df.shape[0])\n",
    "gpt4o_conclusion_recall_ci = 1.96 * np.sqrt((gpt4o_conclusion_recall * (1 - gpt4o_conclusion_recall)) / df.shape[0])\n",
    "gpt4o_conclusion_f1_ci = 1.96 * np.sqrt((gpt4o_conclusion_f1 * (1 - gpt4o_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-4o Conclusion Accuracy: {gpt4o_conclusion_accuracy:.2f} ({gpt4o_conclusion_accuracy - gpt4o_conclusion_accuracy_ci:.2f} - {gpt4o_conclusion_accuracy + gpt4o_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-4o Conclusion Precision: {gpt4o_conclusion_precision:.2f} ({gpt4o_conclusion_precision - gpt4o_conclusion_precision_ci:.2f} - {gpt4o_conclusion_precision + gpt4o_conclusion_precision_ci:.2f})\")\n",
    "print(f\"GPT-4o Conclusion Recall: {gpt4o_conclusion_recall:.2f} ({gpt4o_conclusion_recall - gpt4o_conclusion_recall_ci:.2f} - {gpt4o_conclusion_recall + gpt4o_conclusion_recall_ci:.2f})\")\n",
    "print(f\"GPT-4o Conclusion F1: {gpt4o_conclusion_f1:.2f} ({gpt4o_conclusion_f1 - gpt4o_conclusion_f1_ci:.2f} - {gpt4o_conclusion_f1 + gpt4o_conclusion_f1_ci:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt4o_conclusion = gpt4o_conclusion_crosstab.values.astype(str)\n",
    "labels_gpt4o_conclusion[0, 0] = f\"{labels_gpt4o_conclusion[0, 0]} ({100 * gpt4o_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_conclusion[0, 1] = f\"{labels_gpt4o_conclusion[0, 1]} ({100 * gpt4o_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_conclusion[1, 0] = f\"{labels_gpt4o_conclusion[1, 0]} ({100 * gpt4o_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_conclusion[1, 1] = f\"{labels_gpt4o_conclusion[1, 1]} ({100 * gpt4o_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({'font.size': 14})  # Increase font size\n",
    "sns.heatmap(gpt4o_conclusion_crosstab, annot=labels_gpt4o_conclusion, fmt=\"\", cmap=\"Blues\", annot_kws={'size': 12})\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/gpt4o_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the o1 predictions when reading only the conclusions and the true labels\n",
    "o1_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"o1_temp100_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "o1_conclusion_tp = o1_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "o1_conclusion_fp = o1_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "o1_conclusion_fn = o1_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "o1_conclusion_tn = o1_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "o1_conclusion_accuracy = (o1_conclusion_tp + o1_conclusion_tn) / (o1_conclusion_tp + o1_conclusion_tn + o1_conclusion_fp + o1_conclusion_fn)\n",
    "o1_conclusion_precision = o1_conclusion_tp / (o1_conclusion_tp + o1_conclusion_fp)\n",
    "o1_conclusion_recall = o1_conclusion_tp / (o1_conclusion_tp + o1_conclusion_fn)\n",
    "o1_conclusion_f1 = 2 * (o1_conclusion_precision * o1_conclusion_recall) / (o1_conclusion_precision + o1_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "o1_conclusion_accuracy_ci = 1.96 * np.sqrt((o1_conclusion_accuracy * (1 - o1_conclusion_accuracy)) / df.shape[0])\n",
    "o1_conclusion_precision_ci = 1.96 * np.sqrt((o1_conclusion_precision * (1 - o1_conclusion_precision)) / df.shape[0])\n",
    "o1_conclusion_recall_ci = 1.96 * np.sqrt((o1_conclusion_recall * (1 - o1_conclusion_recall)) / df.shape[0])\n",
    "o1_conclusion_f1_ci = 1.96 * np.sqrt((o1_conclusion_f1 * (1 - o1_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"o1 Conclusion Accuracy: {o1_conclusion_accuracy:.2f} ({o1_conclusion_accuracy - o1_conclusion_accuracy_ci:.2f} - {o1_conclusion_accuracy + o1_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"o1 Conclusion Precision: {o1_conclusion_precision:.2f} ({o1_conclusion_precision - o1_conclusion_precision_ci:.2f} - {o1_conclusion_precision + o1_conclusion_precision_ci:.2f})\")\n",
    "print(f\"o1 Conclusion Recall: {o1_conclusion_recall:.2f} ({o1_conclusion_recall - o1_conclusion_recall_ci:.2f} - {o1_conclusion_recall + o1_conclusion_recall_ci:.2f})\")\n",
    "print(f\"o1 Conclusion F1: {o1_conclusion_f1:.2f} ({o1_conclusion_f1 - o1_conclusion_f1_ci:.2f} - {o1_conclusion_f1 + o1_conclusion_f1_ci:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_o1_conclusion = o1_conclusion_crosstab.values.astype(str)\n",
    "labels_o1_conclusion[0, 0] = f\"{labels_o1_conclusion[0, 0]} ({100 * o1_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_conclusion[0, 1] = f\"{labels_o1_conclusion[0, 1]} ({100 * o1_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_conclusion[1, 0] = f\"{labels_o1_conclusion[1, 0]} ({100 * o1_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_conclusion[1, 1] = f\"{labels_o1_conclusion[1, 1]} ({100 * o1_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({'font.size': 14})  # Increase font size\n",
    "sns.heatmap(o1_conclusion_crosstab, annot=labels_o1_conclusion, fmt=\"\", cmap=\"Blues\", annot_kws={'size': 12})\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/o1_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-3.5 Turbo predictions when reading only the methods/conclusion and the true labels\n",
    "gpt35turbo_methods_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt35turbo_temp100_methods_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "gpt35turbo_methods_conclusion_tp = gpt35turbo_methods_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt35turbo_methods_conclusion_fp = gpt35turbo_methods_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt35turbo_methods_conclusion_fn = gpt35turbo_methods_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt35turbo_methods_conclusion_tn = gpt35turbo_methods_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_methods_conclusion_accuracy = (gpt35turbo_methods_conclusion_tp + gpt35turbo_methods_conclusion_tn) / (gpt35turbo_methods_conclusion_tp + gpt35turbo_methods_conclusion_tn + gpt35turbo_methods_conclusion_fp + gpt35turbo_methods_conclusion_fn)\n",
    "gpt35turbo_methods_conclusion_precision = gpt35turbo_methods_conclusion_tp / (gpt35turbo_methods_conclusion_tp + gpt35turbo_methods_conclusion_fp)\n",
    "gpt35turbo_methods_conclusion_recall = gpt35turbo_methods_conclusion_tp / (gpt35turbo_methods_conclusion_tp + gpt35turbo_methods_conclusion_fn)\n",
    "gpt35turbo_methods_conclusion_f1 = 2 * (gpt35turbo_methods_conclusion_precision * gpt35turbo_methods_conclusion_recall) / (gpt35turbo_methods_conclusion_precision + gpt35turbo_methods_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_methods_conclusion_accuracy_ci = 1.96 * np.sqrt((gpt35turbo_methods_conclusion_accuracy * (1 - gpt35turbo_methods_conclusion_accuracy)) / df.shape[0])\n",
    "gpt35turbo_methods_conclusion_precision_ci = 1.96 * np.sqrt((gpt35turbo_methods_conclusion_precision * (1 - gpt35turbo_methods_conclusion_precision)) / df.shape[0])\n",
    "gpt35turbo_methods_conclusion_recall_ci = 1.96 * np.sqrt((gpt35turbo_methods_conclusion_recall * (1 - gpt35turbo_methods_conclusion_recall)) / df.shape[0])\n",
    "gpt35turbo_methods_conclusion_f1_ci = 1.96 * np.sqrt((gpt35turbo_methods_conclusion_f1 * (1 - gpt35turbo_methods_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-3.5 Turbo Methods and Conclusion Accuracy: {gpt35turbo_methods_conclusion_accuracy:.2f} ({gpt35turbo_methods_conclusion_accuracy - gpt35turbo_methods_conclusion_accuracy_ci:.2f} - {gpt35turbo_methods_conclusion_accuracy + gpt35turbo_methods_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Methods and Conclusion Precision: {gpt35turbo_methods_conclusion_precision:.2f} ({gpt35turbo_methods_conclusion_precision - gpt35turbo_methods_conclusion_precision_ci:.2f} - {gpt35turbo_methods_conclusion_precision + gpt35turbo_methods_conclusion_precision_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Methods and Conclusion Recall: {gpt35turbo_methods_conclusion_recall:.2f} ({gpt35turbo_methods_conclusion_recall - gpt35turbo_methods_conclusion_recall_ci:.2f} - {gpt35turbo_methods_conclusion_recall + gpt35turbo_methods_conclusion_recall_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Methods and Conclusion F1: {gpt35turbo_methods_conclusion_f1:.2f} ({gpt35turbo_methods_conclusion_f1 - gpt35turbo_methods_conclusion_f1_ci:.2f} - {gpt35turbo_methods_conclusion_f1 + gpt35turbo_methods_conclusion_f1_ci:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt35turbo_methods_conclusion = gpt35turbo_methods_conclusion_crosstab.values.astype(str)\n",
    "labels_gpt35turbo_methods_conclusion[0, 0] = f\"{labels_gpt35turbo_methods_conclusion[0, 0]} ({100 * gpt35turbo_methods_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_methods_conclusion[0, 1] = f\"{labels_gpt35turbo_methods_conclusion[0, 1]} ({100 * gpt35turbo_methods_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_methods_conclusion[1, 0] = f\"{labels_gpt35turbo_methods_conclusion[1, 0]} ({100 * gpt35turbo_methods_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_methods_conclusion[1, 1] = f\"{labels_gpt35turbo_methods_conclusion[1, 1]} ({100 * gpt35turbo_methods_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(gpt35turbo_methods_conclusion_crosstab, annot=labels_gpt35turbo_methods_conclusion, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\n",
    "    \"plots/gpt35turbo_methods_conclusion_confusion_matrix.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-4o predictions when reading only the methods/conclusion and the true labels\n",
    "gpt4o_methods_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt4o_temp100_methods_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "gpt4o_methods_conclusion_tp = gpt4o_methods_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt4o_methods_conclusion_fp = gpt4o_methods_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt4o_methods_conclusion_fn = gpt4o_methods_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt4o_methods_conclusion_tn = gpt4o_methods_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt4o_methods_conclusion_accuracy = (gpt4o_methods_conclusion_tp + gpt4o_methods_conclusion_tn) / (gpt4o_methods_conclusion_tp + gpt4o_methods_conclusion_tn + gpt4o_methods_conclusion_fp + gpt4o_methods_conclusion_fn)\n",
    "gpt4o_methods_conclusion_precision = gpt4o_methods_conclusion_tp / (gpt4o_methods_conclusion_tp + gpt4o_methods_conclusion_fp)\n",
    "gpt4o_methods_conclusion_recall = gpt4o_methods_conclusion_tp / (gpt4o_methods_conclusion_tp + gpt4o_methods_conclusion_fn)\n",
    "gpt4o_methods_conclusion_f1 = 2 * (gpt4o_methods_conclusion_precision * gpt4o_methods_conclusion_recall) / (gpt4o_methods_conclusion_precision + gpt4o_methods_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt4o_methods_conclusion_accuracy_ci = 1.96 * np.sqrt((gpt4o_methods_conclusion_accuracy * (1 - gpt4o_methods_conclusion_accuracy)) / df.shape[0])\n",
    "gpt4o_methods_conclusion_precision_ci = 1.96 * np.sqrt((gpt4o_methods_conclusion_precision * (1 - gpt4o_methods_conclusion_precision)) / df.shape[0])\n",
    "gpt4o_methods_conclusion_recall_ci = 1.96 * np.sqrt((gpt4o_methods_conclusion_recall * (1 - gpt4o_methods_conclusion_recall)) / df.shape[0])\n",
    "gpt4o_methods_conclusion_f1_ci = 1.96 * np.sqrt((gpt4o_methods_conclusion_f1 * (1 - gpt4o_methods_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-4o Methods and Conclusion Accuracy: {gpt4o_methods_conclusion_accuracy:.2f} ({gpt4o_methods_conclusion_accuracy - gpt4o_methods_conclusion_accuracy_ci:.2f} - {gpt4o_methods_conclusion_accuracy + gpt4o_methods_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-4o Methods and Conclusion Precision: {gpt4o_methods_conclusion_precision:.2f} ({gpt4o_methods_conclusion_precision - gpt4o_methods_conclusion_precision_ci:.2f} - {gpt4o_methods_conclusion_precision + gpt4o_methods_conclusion_precision_ci:.2f})\")\n",
    "print(f\"GPT-4o Methods and Conclusion Recall: {gpt4o_methods_conclusion_recall:.2f} ({gpt4o_methods_conclusion_recall - gpt4o_methods_conclusion_recall_ci:.2f} - {gpt4o_methods_conclusion_recall + gpt4o_methods_conclusion_recall_ci:.2f})\")\n",
    "print(f\"GPT-4o Methods and Conclusion F1: {gpt4o_methods_conclusion_f1:.2f} ({gpt4o_methods_conclusion_f1 - gpt4o_methods_conclusion_f1_ci:.2f} - {gpt4o_methods_conclusion_f1 + gpt4o_methods_conclusion_f1_ci:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt4o_methods_conclusion = gpt4o_methods_conclusion_crosstab.values.astype(str)\n",
    "labels_gpt4o_methods_conclusion[0, 0] = f\"{labels_gpt4o_methods_conclusion[0, 0]} ({100 * gpt4o_methods_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_methods_conclusion[0, 1] = f\"{labels_gpt4o_methods_conclusion[0, 1]} ({100 * gpt4o_methods_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_methods_conclusion[1, 0] = f\"{labels_gpt4o_methods_conclusion[1, 0]} ({100 * gpt4o_methods_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_methods_conclusion[1, 1] = f\"{labels_gpt4o_methods_conclusion[1, 1]} ({100 * gpt4o_methods_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(gpt4o_methods_conclusion_crosstab, annot=labels_gpt4o_methods_conclusion, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/gpt4o_methods_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the o1 predictions when reading only the methods/conclusion and the true labels\n",
    "o1_methods_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"o1_temp100_methods_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "o1_methods_conclusion_tp = o1_methods_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "o1_methods_conclusion_fp = o1_methods_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "o1_methods_conclusion_fn = o1_methods_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "o1_methods_conclusion_tn = o1_methods_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "o1_methods_conclusion_accuracy = (o1_methods_conclusion_tp + o1_methods_conclusion_tn) / (o1_methods_conclusion_tp + o1_methods_conclusion_tn + o1_methods_conclusion_fp + o1_methods_conclusion_fn)\n",
    "o1_methods_conclusion_precision = o1_methods_conclusion_tp / (o1_methods_conclusion_tp + o1_methods_conclusion_fp)\n",
    "o1_methods_conclusion_recall = o1_methods_conclusion_tp / (o1_methods_conclusion_tp + o1_methods_conclusion_fn)\n",
    "o1_methods_conclusion_f1 = 2 * (o1_methods_conclusion_precision * o1_methods_conclusion_recall) / (o1_methods_conclusion_precision + o1_methods_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "o1_methods_conclusion_accuracy_ci = 1.96 * np.sqrt((o1_methods_conclusion_accuracy * (1 - o1_methods_conclusion_accuracy)) / df.shape[0])\n",
    "o1_methods_conclusion_precision_ci = 1.96 * np.sqrt((o1_methods_conclusion_precision * (1 - o1_methods_conclusion_precision)) / df.shape[0])\n",
    "o1_methods_conclusion_recall_ci = 1.96 * np.sqrt((o1_methods_conclusion_recall * (1 - o1_methods_conclusion_recall)) / df.shape[0])\n",
    "o1_methods_conclusion_f1_ci = 1.96 * np.sqrt((o1_methods_conclusion_f1 * (1 - o1_methods_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"o1 Methods and Conclusion Accuracy: {o1_methods_conclusion_accuracy:.2f} ({o1_methods_conclusion_accuracy - o1_methods_conclusion_accuracy_ci:.2f} - {o1_methods_conclusion_accuracy + o1_methods_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"o1 Methods and Conclusion Precision: {o1_methods_conclusion_precision:.2f} ({o1_methods_conclusion_precision - o1_methods_conclusion_precision_ci:.2f} - {o1_methods_conclusion_precision + o1_methods_conclusion_precision_ci:.2f})\")\n",
    "print(f\"o1 Methods and Conclusion Recall: {o1_methods_conclusion_recall:.2f} ({o1_methods_conclusion_recall - o1_methods_conclusion_recall_ci:.2f} - {o1_methods_conclusion_recall + o1_methods_conclusion_recall_ci:.2f})\")\n",
    "print(f\"o1 Methods and Conclusion F1: {o1_methods_conclusion_f1:.2f} ({o1_methods_conclusion_f1 - o1_methods_conclusion_f1_ci:.2f} - {o1_methods_conclusion_f1 + o1_methods_conclusion_f1_ci:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_o1_methods_conclusion = o1_methods_conclusion_crosstab.values.astype(str)\n",
    "labels_o1_methods_conclusion[0, 0] = f\"{labels_o1_methods_conclusion[0, 0]} ({100 * o1_methods_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_methods_conclusion[0, 1] = f\"{labels_o1_methods_conclusion[0, 1]} ({100 * o1_methods_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_methods_conclusion[1, 0] = f\"{labels_o1_methods_conclusion[1, 0]} ({100 * o1_methods_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_methods_conclusion[1, 1] = f\"{labels_o1_methods_conclusion[1, 1]} ({100 * o1_methods_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(o1_methods_conclusion_crosstab, annot=labels_o1_methods_conclusion, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/o1_methods_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods, Results, and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-3.5 Turbo predictions when reading only the methods/results/conclusion and the true labels\n",
    "gpt35turbo_methods_results_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt35turbo_temp100_methods_results_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "gpt35turbo_methods_results_conclusion_tp = gpt35turbo_methods_results_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt35turbo_methods_results_conclusion_fp = gpt35turbo_methods_results_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt35turbo_methods_results_conclusion_fn = gpt35turbo_methods_results_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt35turbo_methods_results_conclusion_tn = gpt35turbo_methods_results_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_methods_results_conclusion_accuracy = (gpt35turbo_methods_results_conclusion_tp + gpt35turbo_methods_results_conclusion_tn) / (gpt35turbo_methods_results_conclusion_tp + gpt35turbo_methods_results_conclusion_tn + gpt35turbo_methods_results_conclusion_fp + gpt35turbo_methods_results_conclusion_fn)\n",
    "gpt35turbo_methods_results_conclusion_precision = gpt35turbo_methods_results_conclusion_tp / (gpt35turbo_methods_results_conclusion_tp + gpt35turbo_methods_results_conclusion_fp)\n",
    "gpt35turbo_methods_results_conclusion_recall = gpt35turbo_methods_results_conclusion_tp / (gpt35turbo_methods_results_conclusion_tp + gpt35turbo_methods_results_conclusion_fn)\n",
    "gpt35turbo_methods_results_conclusion_f1 = 2 * (gpt35turbo_methods_results_conclusion_precision * gpt35turbo_methods_results_conclusion_recall) / (gpt35turbo_methods_results_conclusion_precision + gpt35turbo_methods_results_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_methods_results_conclusion_accuracy_ci = 1.96 * np.sqrt((gpt35turbo_methods_results_conclusion_accuracy * (1 - gpt35turbo_methods_results_conclusion_accuracy)) / df.shape[0])\n",
    "gpt35turbo_methods_results_conclusion_precision_ci = 1.96 * np.sqrt((gpt35turbo_methods_results_conclusion_precision * (1 - gpt35turbo_methods_results_conclusion_precision)) / df.shape[0])\n",
    "gpt35turbo_methods_results_conclusion_recall_ci = 1.96 * np.sqrt((gpt35turbo_methods_results_conclusion_recall * (1 - gpt35turbo_methods_results_conclusion_recall)) / df.shape[0])\n",
    "gpt35turbo_methods_results_conclusion_f1_ci = 1.96 * np.sqrt((gpt35turbo_methods_results_conclusion_f1 * (1 - gpt35turbo_methods_results_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-3.5 Turbo Methods, Results, and Conclusion Accuracy: {gpt35turbo_methods_results_conclusion_accuracy:.2f} ({gpt35turbo_methods_results_conclusion_accuracy - gpt35turbo_methods_results_conclusion_accuracy_ci:.2f} - {gpt35turbo_methods_results_conclusion_accuracy + gpt35turbo_methods_results_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Methods, Results, and Conclusion Precision: {gpt35turbo_methods_results_conclusion_precision:.2f} ({gpt35turbo_methods_results_conclusion_precision - gpt35turbo_methods_results_conclusion_precision_ci:.2f} - {gpt35turbo_methods_results_conclusion_precision + gpt35turbo_methods_results_conclusion_precision_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Methods, Results, and Conclusion Recall: {gpt35turbo_methods_results_conclusion_recall:.2f} ({gpt35turbo_methods_results_conclusion_recall - gpt35turbo_methods_results_conclusion_recall_ci:.2f} - {gpt35turbo_methods_results_conclusion_recall + gpt35turbo_methods_results_conclusion_recall_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Methods, Results, and Conclusion F1: {gpt35turbo_methods_results_conclusion_f1:.2f} ({gpt35turbo_methods_results_conclusion_f1 - gpt35turbo_methods_results_conclusion_f1_ci:.2f} - {gpt35turbo_methods_results_conclusion_f1 + gpt35turbo_methods_results_conclusion_f1_ci:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt35turbo_methods_results_conclusion = gpt35turbo_methods_results_conclusion_crosstab.values.astype(str)\n",
    "labels_gpt35turbo_methods_results_conclusion[0, 0] = f\"{labels_gpt35turbo_methods_results_conclusion[0, 0]} ({100 * gpt35turbo_methods_results_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_methods_results_conclusion[0, 1] = f\"{labels_gpt35turbo_methods_results_conclusion[0, 1]} ({100 * gpt35turbo_methods_results_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_methods_results_conclusion[1, 0] = f\"{labels_gpt35turbo_methods_results_conclusion[1, 0]} ({100 * gpt35turbo_methods_results_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_methods_results_conclusion[1, 1] = f\"{labels_gpt35turbo_methods_results_conclusion[1, 1]} ({100 * gpt35turbo_methods_results_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(gpt35turbo_methods_results_conclusion_crosstab, annot=labels_gpt35turbo_methods_results_conclusion, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/gpt35turbo_methods_results_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-4o predictions when reading only the methods/results/conclusion and the true labels\n",
    "gpt4o_methods_results_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt4o_temp100_methods_results_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "gpt4o_methods_results_conclusion_tp = gpt4o_methods_results_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt4o_methods_results_conclusion_fp = gpt4o_methods_results_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt4o_methods_results_conclusion_fn = gpt4o_methods_results_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt4o_methods_results_conclusion_tn = gpt4o_methods_results_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt4o_methods_results_conclusion_accuracy = (gpt4o_methods_results_conclusion_tp + gpt4o_methods_results_conclusion_tn) / (gpt4o_methods_results_conclusion_tp + gpt4o_methods_results_conclusion_tn + gpt4o_methods_results_conclusion_fp + gpt4o_methods_results_conclusion_fn)\n",
    "gpt4o_methods_results_conclusion_precision = gpt4o_methods_results_conclusion_tp / (gpt4o_methods_results_conclusion_tp + gpt4o_methods_results_conclusion_fp)\n",
    "gpt4o_methods_results_conclusion_recall = gpt4o_methods_results_conclusion_tp / (gpt4o_methods_results_conclusion_tp + gpt4o_methods_results_conclusion_fn)\n",
    "gpt4o_methods_results_conclusion_f1 = 2 * (gpt4o_methods_results_conclusion_precision * gpt4o_methods_results_conclusion_recall) / (gpt4o_methods_results_conclusion_precision + gpt4o_methods_results_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt4o_methods_results_conclusion_accuracy_ci = 1.96 * np.sqrt((gpt4o_methods_results_conclusion_accuracy * (1 - gpt4o_methods_results_conclusion_accuracy)) / df.shape[0])\n",
    "gpt4o_methods_results_conclusion_precision_ci = 1.96 * np.sqrt((gpt4o_methods_results_conclusion_precision * (1 - gpt4o_methods_results_conclusion_precision)) / df.shape[0])\n",
    "gpt4o_methods_results_conclusion_recall_ci = 1.96 * np.sqrt((gpt4o_methods_results_conclusion_recall * (1 - gpt4o_methods_results_conclusion_recall)) / df.shape[0])\n",
    "gpt4o_methods_results_conclusion_f1_ci = 1.96 * np.sqrt((gpt4o_methods_results_conclusion_f1 * (1 - gpt4o_methods_results_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-4o Methods, Results, and Conclusion Accuracy: {gpt4o_methods_results_conclusion_accuracy:.2f} ({gpt4o_methods_results_conclusion_accuracy - gpt4o_methods_results_conclusion_accuracy_ci:.2f} - {gpt4o_methods_results_conclusion_accuracy + gpt4o_methods_results_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-4o Methods, Results, and Conclusion Precision: {gpt4o_methods_results_conclusion_precision:.2f} ({gpt4o_methods_results_conclusion_precision - gpt4o_methods_results_conclusion_precision_ci:.2f} - {gpt4o_methods_results_conclusion_precision + gpt4o_methods_results_conclusion_precision_ci:.2f})\")\n",
    "print(f\"GPT-4o Methods, Results, and Conclusion Recall: {gpt4o_methods_results_conclusion_recall:.2f} ({gpt4o_methods_results_conclusion_recall - gpt4o_methods_results_conclusion_recall_ci:.2f} - {gpt4o_methods_results_conclusion_recall + gpt4o_methods_results_conclusion_recall_ci:.2f})\")\n",
    "print(f\"GPT-4o Methods, Results, and Conclusion F1: {gpt4o_methods_results_conclusion_f1:.2f} ({gpt4o_methods_results_conclusion_f1 - gpt4o_methods_results_conclusion_f1_ci:.2f} - {gpt4o_methods_results_conclusion_f1 + gpt4o_methods_results_conclusion_f1_ci:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt4o_methods_results_conclusion = gpt4o_methods_results_conclusion_crosstab.values.astype(str)\n",
    "labels_gpt4o_methods_results_conclusion[0, 0] = f\"{labels_gpt4o_methods_results_conclusion[0, 0]} ({100 * gpt4o_methods_results_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_methods_results_conclusion[0, 1] = f\"{labels_gpt4o_methods_results_conclusion[0, 1]} ({100 * gpt4o_methods_results_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_methods_results_conclusion[1, 0] = f\"{labels_gpt4o_methods_results_conclusion[1, 0]} ({100 * gpt4o_methods_results_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_methods_results_conclusion[1, 1] = f\"{labels_gpt4o_methods_results_conclusion[1, 1]} ({100 * gpt4o_methods_results_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(gpt4o_methods_results_conclusion_crosstab, annot=labels_gpt4o_methods_results_conclusion, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/gpt4o_methods_results_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-4o predictions when reading only the methods/results/conclusion and the true labels\n",
    "o1_methods_results_conclusion_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"o1_temp100_methods_results_conclusion_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "o1_methods_results_conclusion_tp = o1_methods_results_conclusion_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "o1_methods_results_conclusion_fp = o1_methods_results_conclusion_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "o1_methods_results_conclusion_fn = o1_methods_results_conclusion_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "o1_methods_results_conclusion_tn = o1_methods_results_conclusion_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "o1_methods_results_conclusion_accuracy = (o1_methods_results_conclusion_tp + o1_methods_results_conclusion_tn) / (o1_methods_results_conclusion_tp + o1_methods_results_conclusion_tn + o1_methods_results_conclusion_fp + o1_methods_results_conclusion_fn)\n",
    "o1_methods_results_conclusion_precision = o1_methods_results_conclusion_tp / (o1_methods_results_conclusion_tp + o1_methods_results_conclusion_fp)\n",
    "o1_methods_results_conclusion_recall = o1_methods_results_conclusion_tp / (o1_methods_results_conclusion_tp + o1_methods_results_conclusion_fn)\n",
    "o1_methods_results_conclusion_f1 = 2 * (o1_methods_results_conclusion_precision * o1_methods_results_conclusion_recall) / (o1_methods_results_conclusion_precision + o1_methods_results_conclusion_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "o1_methods_results_conclusion_accuracy_ci = 1.96 * np.sqrt((o1_methods_results_conclusion_accuracy * (1 - o1_methods_results_conclusion_accuracy)) / df.shape[0])\n",
    "o1_methods_results_conclusion_precision_ci = 1.96 * np.sqrt((o1_methods_results_conclusion_precision * (1 - o1_methods_results_conclusion_precision)) / df.shape[0])\n",
    "o1_methods_results_conclusion_recall_ci = 1.96 * np.sqrt((o1_methods_results_conclusion_recall * (1 - o1_methods_results_conclusion_recall)) / df.shape[0])\n",
    "o1_methods_results_conclusion_f1_ci = 1.96 * np.sqrt((o1_methods_results_conclusion_f1 * (1 - o1_methods_results_conclusion_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"o1 Methods, Results, and Conclusion Accuracy: {o1_methods_results_conclusion_accuracy:.2f} ({o1_methods_results_conclusion_accuracy - o1_methods_results_conclusion_accuracy_ci:.2f} - {o1_methods_results_conclusion_accuracy + o1_methods_results_conclusion_accuracy_ci:.2f})\")\n",
    "print(f\"o1 Methods, Results, and Conclusion Precision: {o1_methods_results_conclusion_precision:.2f} ({o1_methods_results_conclusion_precision - o1_methods_results_conclusion_precision_ci:.2f} - {o1_methods_results_conclusion_precision + o1_methods_results_conclusion_precision_ci:.2f})\")\n",
    "print(f\"o1 Methods, Results, and Conclusion Recall: {o1_methods_results_conclusion_recall:.2f} ({o1_methods_results_conclusion_recall - o1_methods_results_conclusion_recall_ci:.2f} - {o1_methods_results_conclusion_recall + o1_methods_results_conclusion_recall_ci:.2f})\")\n",
    "print(f\"o1 Methods, Results, and Conclusion F1: {o1_methods_results_conclusion_f1:.2f} ({o1_methods_results_conclusion_f1 - o1_methods_results_conclusion_f1_ci:.2f} - {o1_methods_results_conclusion_f1 + o1_methods_results_conclusion_f1_ci:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_o1_methods_results_conclusion = o1_methods_results_conclusion_crosstab.values.astype(str)\n",
    "labels_o1_methods_results_conclusion[0, 0] = f\"{labels_o1_methods_results_conclusion[0, 0]} ({100 * o1_methods_results_conclusion_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_methods_results_conclusion[0, 1] = f\"{labels_o1_methods_results_conclusion[0, 1]} ({100 * o1_methods_results_conclusion_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_methods_results_conclusion[1, 0] = f\"{labels_o1_methods_results_conclusion[1, 0]} ({100 * o1_methods_results_conclusion_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_methods_results_conclusion[1, 1] = f\"{labels_o1_methods_results_conclusion[1, 1]} ({100 * o1_methods_results_conclusion_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(o1_methods_results_conclusion_crosstab, annot=labels_o1_methods_results_conclusion, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/o1_methods_results_conclusion_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title and Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3.5 Turbo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-3.5 Turbo predictions when reading the title/abstract and the true labels\n",
    "gpt35turbo_title_abstract_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt35turbo_temp100_title_abstract_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "gpt35turbo_title_abstract_tp = gpt35turbo_title_abstract_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt35turbo_title_abstract_fp = gpt35turbo_title_abstract_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt35turbo_title_abstract_fn = gpt35turbo_title_abstract_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt35turbo_title_abstract_tn = gpt35turbo_title_abstract_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_title_abstract_accuracy = (gpt35turbo_title_abstract_tp + gpt35turbo_title_abstract_tn) / (gpt35turbo_title_abstract_tp + gpt35turbo_title_abstract_tn + gpt35turbo_title_abstract_fp + gpt35turbo_title_abstract_fn)\n",
    "gpt35turbo_title_abstract_precision = gpt35turbo_title_abstract_tp / (gpt35turbo_title_abstract_tp + gpt35turbo_title_abstract_fp)\n",
    "gpt35turbo_title_abstract_recall = gpt35turbo_title_abstract_tp / (gpt35turbo_title_abstract_tp + gpt35turbo_title_abstract_fn)\n",
    "gpt35turbo_title_abstract_f1 = 2 * (gpt35turbo_title_abstract_precision * gpt35turbo_title_abstract_recall) / (gpt35turbo_title_abstract_precision + gpt35turbo_title_abstract_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt35turbo_title_abstract_accuracy_ci = 1.96 * np.sqrt((gpt35turbo_title_abstract_accuracy * (1 - gpt35turbo_title_abstract_accuracy)) / df.shape[0])\n",
    "gpt35turbo_title_abstract_precision_ci = 1.96 * np.sqrt((gpt35turbo_title_abstract_precision * (1 - gpt35turbo_title_abstract_precision)) / df.shape[0])\n",
    "gpt35turbo_title_abstract_recall_ci = 1.96 * np.sqrt((gpt35turbo_title_abstract_recall * (1 - gpt35turbo_title_abstract_recall)) / df.shape[0])\n",
    "gpt35turbo_title_abstract_f1_ci = 1.96 * np.sqrt((gpt35turbo_title_abstract_f1 * (1 - gpt35turbo_title_abstract_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-3.5 Turbo Title and Abstract Accuracy: {gpt35turbo_title_abstract_accuracy:.2f} ({gpt35turbo_title_abstract_accuracy - gpt35turbo_title_abstract_accuracy_ci:.2f} - {gpt35turbo_title_abstract_accuracy + gpt35turbo_title_abstract_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Title and Abstract Precision: {gpt35turbo_title_abstract_precision:.2f} ({gpt35turbo_title_abstract_precision - gpt35turbo_title_abstract_precision_ci:.2f} - {gpt35turbo_title_abstract_precision + gpt35turbo_title_abstract_precision_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Title and Abstract Recall: {gpt35turbo_title_abstract_recall:.2f} ({gpt35turbo_title_abstract_recall - gpt35turbo_title_abstract_recall_ci:.2f} - {gpt35turbo_title_abstract_recall + gpt35turbo_title_abstract_recall_ci:.2f})\")\n",
    "print(f\"GPT-3.5 Turbo Title and Abstract F1: {gpt35turbo_title_abstract_f1:.2f} ({gpt35turbo_title_abstract_f1 - gpt35turbo_title_abstract_f1_ci:.2f} - {gpt35turbo_title_abstract_f1 + gpt35turbo_title_abstract_f1_ci:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt35turbo_title_abstract = gpt35turbo_title_abstract_crosstab.values.astype(str)\n",
    "labels_gpt35turbo_title_abstract[0, 0] = f\"{labels_gpt35turbo_title_abstract[0, 0]} ({100 * gpt35turbo_title_abstract_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_title_abstract[0, 1] = f\"{labels_gpt35turbo_title_abstract[0, 1]} ({100 * gpt35turbo_title_abstract_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_title_abstract[1, 0] = f\"{labels_gpt35turbo_title_abstract[1, 0]} ({100 * gpt35turbo_title_abstract_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt35turbo_title_abstract[1, 1] = f\"{labels_gpt35turbo_title_abstract[1, 1]} ({100 * gpt35turbo_title_abstract_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(gpt35turbo_title_abstract_crosstab, annot=labels_gpt35turbo_title_abstract, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/gpt35turbo_title_abstract_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the GPT-4o predictions when reading the title/abstract and the true labels\n",
    "gpt4o_title_abstract_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"gpt4o_temp100_title_abstract_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "gpt4o_title_abstract_tp = gpt4o_title_abstract_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "gpt4o_title_abstract_fp = gpt4o_title_abstract_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "gpt4o_title_abstract_fn = gpt4o_title_abstract_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "gpt4o_title_abstract_tn = gpt4o_title_abstract_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "gpt4o_title_abstract_accuracy = (gpt4o_title_abstract_tp + gpt4o_title_abstract_tn) / (gpt4o_title_abstract_tp + gpt4o_title_abstract_tn + gpt4o_title_abstract_fp + gpt4o_title_abstract_fn)\n",
    "gpt4o_title_abstract_precision = gpt4o_title_abstract_tp / (gpt4o_title_abstract_tp + gpt4o_title_abstract_fp)\n",
    "gpt4o_title_abstract_recall = gpt4o_title_abstract_tp / (gpt4o_title_abstract_tp + gpt4o_title_abstract_fn)\n",
    "gpt4o_title_abstract_f1 = 2 * (gpt4o_title_abstract_precision * gpt4o_title_abstract_recall) / (gpt4o_title_abstract_precision + gpt4o_title_abstract_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "gpt4o_title_abstract_accuracy_ci = 1.96 * np.sqrt((gpt4o_title_abstract_accuracy * (1 - gpt4o_title_abstract_accuracy)) / df.shape[0])\n",
    "gpt4o_title_abstract_precision_ci = 1.96 * np.sqrt((gpt4o_title_abstract_precision * (1 - gpt4o_title_abstract_precision)) / df.shape[0])\n",
    "gpt4o_title_abstract_recall_ci = 1.96 * np.sqrt((gpt4o_title_abstract_recall * (1 - gpt4o_title_abstract_recall)) / df.shape[0])\n",
    "gpt4o_title_abstract_f1_ci = 1.96 * np.sqrt((gpt4o_title_abstract_f1 * (1 - gpt4o_title_abstract_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"GPT-4o Title and Abstract Accuracy: {gpt4o_title_abstract_accuracy:.2f} ({gpt4o_title_abstract_accuracy - gpt4o_title_abstract_accuracy_ci:.2f} - {gpt4o_title_abstract_accuracy + gpt4o_title_abstract_accuracy_ci:.2f})\")\n",
    "print(f\"GPT-4o Title and Abstract Precision: {gpt4o_title_abstract_precision:.2f} ({gpt4o_title_abstract_precision - gpt4o_title_abstract_precision_ci:.2f} - {gpt4o_title_abstract_precision + gpt4o_title_abstract_precision_ci:.2f})\")\n",
    "print(f\"GPT-4o Title and Abstract Recall: {gpt4o_title_abstract_recall:.2f} ({gpt4o_title_abstract_recall - gpt4o_title_abstract_recall_ci:.2f} - {gpt4o_title_abstract_recall + gpt4o_title_abstract_recall_ci:.2f})\")\n",
    "print(f\"GPT-4o Title and Abstract F1: {gpt4o_title_abstract_f1:.2f} ({gpt4o_title_abstract_f1 - gpt4o_title_abstract_f1_ci:.2f} - {gpt4o_title_abstract_f1 + gpt4o_title_abstract_f1_ci:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_gpt4o_title_abstract = gpt4o_title_abstract_crosstab.values.astype(str)\n",
    "labels_gpt4o_title_abstract[0, 0] = f\"{labels_gpt4o_title_abstract[0, 0]} ({100 * gpt4o_title_abstract_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_title_abstract[0, 1] = f\"{labels_gpt4o_title_abstract[0, 1]} ({100 * gpt4o_title_abstract_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_title_abstract[1, 0] = f\"{labels_gpt4o_title_abstract[1, 0]} ({100 * gpt4o_title_abstract_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_gpt4o_title_abstract[1, 1] = f\"{labels_gpt4o_title_abstract[1, 1]} ({100 * gpt4o_title_abstract_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(gpt4o_title_abstract_crosstab, annot=labels_gpt4o_title_abstract, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/gpt4o_title_abstract_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of the o1 predictions when reading the title/abstract and the true labels\n",
    "o1_title_abstract_crosstab = pd.crosstab(\n",
    "    df[\"Annotation_accept\"], df[\"o1_temp100_title_abstract_response_raw\"]\n",
    ")\n",
    "\n",
    "# Calculate the true positives, false positives, false negatives, and true negatives\n",
    "o1_title_abstract_tp = o1_title_abstract_crosstab.loc[\"Positive\", \"Positive\"]\n",
    "o1_title_abstract_fp = o1_title_abstract_crosstab.loc[\"Negative\", \"Positive\"]\n",
    "o1_title_abstract_fn = o1_title_abstract_crosstab.loc[\"Positive\", \"Negative\"]\n",
    "o1_title_abstract_tn = o1_title_abstract_crosstab.loc[\"Negative\", \"Negative\"]\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "o1_title_abstract_accuracy = (o1_title_abstract_tp + o1_title_abstract_tn) / (o1_title_abstract_tp + o1_title_abstract_tn + o1_title_abstract_fp + o1_title_abstract_fn)\n",
    "o1_title_abstract_precision = o1_title_abstract_tp / (o1_title_abstract_tp + o1_title_abstract_fp)\n",
    "o1_title_abstract_recall = o1_title_abstract_tp / (o1_title_abstract_tp + o1_title_abstract_fn)\n",
    "o1_title_abstract_f1 = 2 * (o1_title_abstract_precision * o1_title_abstract_recall) / (o1_title_abstract_precision + o1_title_abstract_recall)\n",
    "\n",
    "# Calculate the 95% confidence interval for the accuracy, precision, recall, and F1 score\n",
    "o1_title_abstract_accuracy_ci = 1.96 * np.sqrt((o1_title_abstract_accuracy * (1 - o1_title_abstract_accuracy)) / df.shape[0])\n",
    "o1_title_abstract_precision_ci = 1.96 * np.sqrt((o1_title_abstract_precision * (1 - o1_title_abstract_precision)) / df.shape[0])\n",
    "o1_title_abstract_recall_ci = 1.96 * np.sqrt((o1_title_abstract_recall * (1 - o1_title_abstract_recall)) / df.shape[0])\n",
    "o1_title_abstract_f1_ci = 1.96 * np.sqrt((o1_title_abstract_f1 * (1 - o1_title_abstract_f1)) / df.shape[0])\n",
    "\n",
    "# Print the results\n",
    "print(f\"o1 Title and Abstract Accuracy: {o1_title_abstract_accuracy:.2f} ({o1_title_abstract_accuracy - o1_title_abstract_accuracy_ci:.2f} - {o1_title_abstract_accuracy + o1_title_abstract_accuracy_ci:.2f})\")\n",
    "print(f\"o1 Title and Abstract Precision: {o1_title_abstract_precision:.2f} ({o1_title_abstract_precision - o1_title_abstract_precision_ci:.2f} - {o1_title_abstract_precision + o1_title_abstract_precision_ci:.2f})\")\n",
    "print(f\"o1 Title and Abstract Recall: {o1_title_abstract_recall:.2f} ({o1_title_abstract_recall - o1_title_abstract_recall_ci:.2f} - {o1_title_abstract_recall + o1_title_abstract_recall_ci:.2f})\")\n",
    "print(f\"o1 Title and Abstract F1: {o1_title_abstract_f1:.2f} ({o1_title_abstract_f1 - o1_title_abstract_f1_ci:.2f} - {o1_title_abstract_f1 + o1_title_abstract_f1_ci:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Create the labels with counts and percentages\n",
    "labels_o1_title_abstract = o1_title_abstract_crosstab.values.astype(str)\n",
    "labels_o1_title_abstract[0, 0] = f\"{labels_o1_title_abstract[0, 0]} ({100 * o1_title_abstract_tn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_title_abstract[0, 1] = f\"{labels_o1_title_abstract[0, 1]} ({100 * o1_title_abstract_fp / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_title_abstract[1, 0] = f\"{labels_o1_title_abstract[1, 0]} ({100 * o1_title_abstract_fn / (df.shape[0]):.1f}%)\"\n",
    "labels_o1_title_abstract[1, 1] = f\"{labels_o1_title_abstract[1, 1]} ({100 * o1_title_abstract_tp / (df.shape[0]):.1f}%)\"\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.rcParams.update({\"font.size\": 14})  # Increase font size\n",
    "sns.heatmap(o1_title_abstract_crosstab, annot=labels_o1_title_abstract, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Ground truth\", fontsize=14)\n",
    "# Remove the colorbar but keep the width of the heatmap\n",
    "plt.gca().collections[0].colorbar.remove()\n",
    "plt.savefig(\"plots/o1_title_abstract_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials with false positive prediction by o1 based on conclusion but correct based on title/abstract: 10\n",
      "['10.1200/JCO.20.02529', '10.1200/JCO.2015.62.1474', '10.1200/JCO.2005.03.0551', '10.1200/JCO.2012.44.7920', '10.1200/JCO.22.01805', '10.1016/S0140-6736(10)62312-4', '10.1200/JCO.2005.05.112', '10.1016/S1470-2045(13)70539-4', '10.1200/JCO.2006.06.0483', '10.1200/JCO.2015.62.4734']\n"
     ]
    }
   ],
   "source": [
    "# Get the false positive predictions by o1 based on the conclusion\n",
    "o1_conclusion_false_positives = df[(df['o1_temp100_conclusion_response_raw'] == \"Positive\") & (df['Annotation_accept'] == \"Negative\")]\n",
    "\n",
    "\n",
    "# Get only the files that were false positives based on the conclusion but correct based on the title/abstract\n",
    "o1_conclusion_fp_title_abstract_correct = o1_conclusion_false_positives[o1_conclusion_false_positives['o1_temp100_title_abstract_response_raw'] == \"Negative\"]\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of trials with false positive prediction by o1 based on conclusion but correct based on title/abstract: {len(o1_conclusion_fp_title_abstract_correct)}\")\n",
    "\n",
    "# Display the dois of the trials\n",
    "print(o1_conclusion_fp_title_abstract_correct['doi'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if true negatives have same patterns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.1016/S1470-2045(18)30193-1', '10.1200/JCO.2012.43.8820', '10.1016/S1470-2045(13)70510-2', '10.1200/JCO.2005.05.098', '10.1200/JCO.2005.04.5252', '10.1001/jamaoncol.2020.4574', '10.1200/JCO.20.02824', '10.1001/jamaoncol.2017.1269', '10.1001/jama.2014.2626', '10.1200/JCO.2015.61.4578']\n"
     ]
    }
   ],
   "source": [
    "# Get 20 true negative predictions by o1 based on the conclusion\n",
    "o1_conclusion_true_negatives = df[(df['o1_temp100_conclusion_response_raw'] == \"Negative\") & (df['Annotation_accept'] == \"Negative\")].sample(10, random_state=1)\n",
    "\n",
    "# Display the dois of the trials\n",
    "print(o1_conclusion_true_negatives['doi'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "journal_NLP_publications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
